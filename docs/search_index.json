[
["index.html", "21 Recipes for Mining Twitter Data with rtweet Preface", " 21 Recipes for Mining Twitter Data with rtweet Bob Rudis 2018-01-02 Preface I’m using this as way to familiarize myself with bookdown so I don’t make as many mistakes with my web scraping field guide book. It’s based on Matthew R. Russell’s book. That book is out of distribution and much of the content is in Matthew’s “Mining the Social Web” book. There will be many similarities between his “21 Recipes” book and this book on purpose. I am not claiming originality in this work, just making an R-centric version of the cookbook. As he states in his tome, “this intentionally terse recipe collection provides you with 21 easily adaptable Twitter mining recipes”. The recipes contained in this book use the rtweet package by Michael W. Kearney. I’ll be using the GitHub version of the package since it has some cutting-edge features and bug-fixes in it. You can install the GitHub version of [rtweet] by first installing the devtools package via: install.packages(&quot;devtools&quot;) then installing the GitHub rtweet package via: devtools::install_github(&quot;mkearney/rtweet&quot;) NOTE: If you try to run examples in this book and receive an error about a package not being found or not available, you’ll need to triage it by using one of the above methods. If any GitHub packages are used, each initial library() call will have a comment after it noting which repository/packagename to use the devtools method with. Matthew also states that “one other thing you should consider doing up front, if you haven’t already, is quickly skimming through the official Twitter API documentation and related development documents linked on that page. Twitter has a very easy-to-use API with a lot of degrees of freedom”. Michael has documented rtweet well, but reading the official documentation will really help. This book also makes extensive use of the tidyverse meta-package. You will need to: install.packages(&quot;tidyverse&quot;) if you have not used packages from it before (it may take a few minutes, especially on Linux systems). "],
["using-oauth-to-access-twitter-apis.html", "Recipe 1 Using OAuth to Access Twitter APIs 1.1 Problem 1.2 Solution 1.3 Discussion 1.4 See Also", " Recipe 1 Using OAuth to Access Twitter APIs 1.1 Problem You want to access your own data or another user’s data for analysis. 1.2 Solution Take advantage of Twitter’s OAuth implementation to gain full access to Twitter’s entire API. 1.3 Discussion Twitter uses OAuth Core 1.0 Revision A (“OAuth 1.0a” for short &amp; to further reduce verbosity, “oauth” from now on). A few, key purposes of oauth in the context of Twitter are: to ensure end-users know an application is registered with Twitter, and know who the author(s) fo the application are; enable limiting what operations an application can perform with your Twitter account; obviate the need to share your actual Twitter username and password with a third party, which also enables recovation of application access to your Twitter account without resetting your password. The rtweet package takes this one step further by having you create an “application”, which is nothing more than you setting up some basic configuration information. To do so, you must visit apps.twitter.com and create a new application. You will need to provide values for the following fields: Name : something you’ll remember Description : another place you can remind yourself what this is for Website : something that points to information you can use to associate this app when you’ve forgotten about it 5 years from now Callback URL : This must be http://127.0.0.1:1410 (we’ll see why in a moment) tick the agreement checkbox Once you submit that form, you’ll see a new page: Select the “Keys and Access Tokens” tab to see important information you’ll need: From the previous page and this page, you’ll need the: Application Name (which is my_rtweet_application in this example but you need to use the one you supplied) Consumer Key (API Key) (which is akNTqsfSjJFQse1c55Vrm6BcZ in this example but you need to use your own) Consumer Secret (API Secret) (which is HFF77rxG5HTx4Ui7RbxYVjoyUup5h0ncls92Q88ddE0n4YFJZN in this example, but — again — you need to use your own) Store both of those in your ~/.Renviron file. If you’re unfamiliar with how to do that, see this handy section from “Efficient R Programming”. I prefer storing these as such: TWITTER_APP=my_rtweet_application TWITTER_CONSUMER_KEY=akNTqsfSjJFQse1c55Vrm6BcZ TWITTER_CONSUMER_SECRET=HFF77rxG5HTx4Ui7RbxYVjoyUup5h0ncls92Q88ddE0n4YFJZN By storing these values in ~/.Renviron you avoid exposing them in subdirectories or within scripts and will always be able to reference them. Now you can enable your Twitter account with this application and create a token: create_token( app = Sys.getenv(&quot;TWITTER_APP&quot;), consumer_key = Sys.getenv(&quot;TWITTER_CONSUMER_KEY&quot;), consumer_secret = Sys.getenv(&quot;TWITTER_CONSUMER_SECRET&quot;) ) -&gt; twitter_token You should see a browser window appear that has an authorization form in it: You’ll also see: Waiting for authentication in browser... Press Esc/Ctrl + C to abort in the R console. The rtweet package used httr to send an oauth request to Twitter and then started up a local web server (this is why that weird localhost URL from before is necessary). When you authorize the application, the browser sends a response back to the web server httr spun up with some important, secret information that will make it possible for you to never have to do this oauth dance again. If everything was successful, you’ll see: Authentication complete. Please close this page and return to R. in the browser window, and: Authentication complete. in the R console. The next step is very important. Save the secret token you just received this way: saveRDS(twitter_token, &quot;~/.rtweet.rds&quot;) then create one more environment variable in ~/.Renviron: TWITTER_PAT=~/.rtweet.rds That last step will help ensure you never have to deal with oauth again (until you want to). Keep this token file safe!! It enables anyone who has it to do virtually anything with your account. If you believe it has been exposed, go back to apps.twitter.com and delete the application (you can also choose to regenerate the Consumer Key and Consumer Secret, but it’s often easier to just make a new application). You should also review your Twitter apps and ensure it’s removed from there as well. Use the Revoke access button if it is: 1.4 See Also The official rtweet authentication vignette "],
["looking-up-the-trending-topics.html", "Recipe 2 Looking Up the Trending Topics 2.1 Problem 2.2 Solution 2.3 Discussion 2.4 See Also", " Recipe 2 Looking Up the Trending Topics 2.1 Problem You want to keep track of the trending topics on Twitter over a period of time. 2.2 Solution Use rtweet::trends_available() to see trend areas and rtweet::get_trends() to pull trends, after which you can setup a task to retrieve and cache the trend data periodically. 2.3 Discussion Twitter has extensive information on trending topics and their API enables you to see topics that are trending globally or regionally. Twitter uses Yahoo! Where on Earth identifiers (WOEIDs) for the regions which can be obtained from rtweet::trends_available(): library(rtweet) library(tidyverse) (trends_avail &lt;- trends_available()) ## # A tibble: 467 x 8 ## name url parentid ## * &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Worldwide http://where.yahooapis.com/v1/place/1 0 ## 2 Winnipeg http://where.yahooapis.com/v1/place/2972 23424775 ## 3 Ottawa http://where.yahooapis.com/v1/place/3369 23424775 ## 4 Quebec http://where.yahooapis.com/v1/place/3444 23424775 ## 5 Montreal http://where.yahooapis.com/v1/place/3534 23424775 ## 6 Toronto http://where.yahooapis.com/v1/place/4118 23424775 ## 7 Edmonton http://where.yahooapis.com/v1/place/8676 23424775 ## 8 Calgary http://where.yahooapis.com/v1/place/8775 23424775 ## 9 Vancouver http://where.yahooapis.com/v1/place/9807 23424775 ## 10 Birmingham http://where.yahooapis.com/v1/place/12723 23424975 ## # ... with 457 more rows, and 5 more variables: country &lt;chr&gt;, ## # woeid &lt;int&gt;, countryCode &lt;chr&gt;, code &lt;int&gt;, place_type &lt;chr&gt; glimpse(trends_avail) ## Observations: 467 ## Variables: 8 ## $ name &lt;chr&gt; &quot;Worldwide&quot;, &quot;Winnipeg&quot;, &quot;Ottawa&quot;, &quot;Quebec&quot;, &quot;Mont... ## $ url &lt;chr&gt; &quot;http://where.yahooapis.com/v1/place/1&quot;, &quot;http://w... ## $ parentid &lt;int&gt; 0, 23424775, 23424775, 23424775, 23424775, 2342477... ## $ country &lt;chr&gt; &quot;&quot;, &quot;Canada&quot;, &quot;Canada&quot;, &quot;Canada&quot;, &quot;Canada&quot;, &quot;Canad... ## $ woeid &lt;int&gt; 1, 2972, 3369, 3444, 3534, 4118, 8676, 8775, 9807,... ## $ countryCode &lt;chr&gt; NA, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;... ## $ code &lt;int&gt; 19, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7... ## $ place_type &lt;chr&gt; &quot;Supername&quot;, &quot;Town&quot;, &quot;Town&quot;, &quot;Town&quot;, &quot;Town&quot;, &quot;Town... The Twitter API is somewhat unforgiving and unfriendly when you use it directly since it requires the use of a WOEID. Michael has made life much easier for us all by enabling the use of names or regular expressions when asking for trends from a particular place. That means we don’t even need to care about capitalization: (us &lt;- get_trends(&quot;united states&quot;)) ## # A tibble: 50 x 9 ## trend ## * &lt;chr&gt; ## 1 #backtowork ## 2 #TuesdayThoughts ## 3 #SavannahHodaTODAY ## 4 Justin Timberlake ## 5 #MyTVShowWasCanceledBecause ## 6 #AM2DM ## 7 Carrie Underwood ## 8 The Trump Effect ## 9 Sean Ryan ## 10 Micah Parsons ## # ... with 40 more rows, and 8 more variables: url &lt;chr&gt;, ## # promoted_content &lt;lgl&gt;, query &lt;chr&gt;, tweet_volume &lt;int&gt;, place &lt;chr&gt;, ## # woeid &lt;int&gt;, as_of &lt;dttm&gt;, created_at &lt;dttm&gt; glimpse(us) ## Observations: 50 ## Variables: 9 ## $ trend &lt;chr&gt; &quot;#backtowork&quot;, &quot;#TuesdayThoughts&quot;, &quot;#Savannah... ## $ url &lt;chr&gt; &quot;http://twitter.com/search?q=%23backtowork&quot;, ... ## $ promoted_content &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N... ## $ query &lt;chr&gt; &quot;%23backtowork&quot;, &quot;%23TuesdayThoughts&quot;, &quot;%23Sa... ## $ tweet_volume &lt;int&gt; 27544, 30822, 10279, NA, NA, NA, NA, 12000, N... ## $ place &lt;chr&gt; &quot;United States&quot;, &quot;United States&quot;, &quot;United Sta... ## $ woeid &lt;int&gt; 23424977, 23424977, 23424977, 23424977, 23424... ## $ as_of &lt;dttm&gt; 2018-01-02 16:34:11, 2018-01-02 16:34:11, 20... ## $ created_at &lt;dttm&gt; 2018-01-02 16:27:46, 2018-01-02 16:27:46, 20... Twitter’s documentation states that trends are updated every 5 minutes, which means you should not call the API more frequently than that and their current API rate-limit (Twitter puts some restrictions on how frequently you can call certain API targets) is 75 requests per 15-minute window. The rtweet::get_trends() function returns a data frame. Our ultimate goal is to retrieve the trends data on a schedule and cache it. There are numerous — and usually complex – ways to schedule jobs. One cross-platform solution is to use R itself to run a task periodically. This means keeping an R console open and running at all times, so is far from an optimal solution. See the taskscheduleR package for other ideas on how to setup more robust scheduled jobs. In this example, we will: use a SQLite database to store the trends use the DBI add RSQlite packages to work with this database setup a never-ending loop with Sys.sleep() providing a pause between requests library(DBI) library(RSQLite) library(rtweet) # mkearney/rtweet repeat { message(&quot;Retrieveing trends...&quot;) # optional us &lt;- get_trends(&quot;united states&quot;) db_con &lt;- dbConnect(RSQLite::SQLite(), &quot;data/us-trends.db&quot;) dbWriteTable(db_con, &quot;us_trends&quot;, us, append=TRUE) # append=TRUE will update the table vs overwrite and also create it on first run if it does not exist dbDisconnect(db_con) Sys.sleep(10 * 60) # sleep for 10 minutes } Later on, we can look at this data with dplyr/dbplyr: library(dplyr) trends_db &lt;- src_sqlite(&quot;data/us-trends.db&quot;) us &lt;- tbl(trends_db, &quot;us_trends&quot;) select(us, trend) ## # Source: lazy query [?? x 1] ## # Database: sqlite 3.19.3 ## # [/Users/hrbrmstr/Development/21-recipes/data/us-trends.db] ## trend ## &lt;chr&gt; ## 1 #TuesdayThoughts ## 2 #backtowork ## 3 #SavannahHodaTODAY ## 4 Justin Timberlake ## 5 #MyTVShowWasCanceledBecause ## 6 #AM2DM ## 7 The Trump Effect ## 8 Carrie Underwood ## 9 Sean Ryan ## 10 Larry Krasner ## # ... with more rows 2.4 See Also RSQlite quick reference Introduction to dbplyr : http://dbplyr.tidyverse.org/articles/dbplyr.html "],
["extracting-tweet-entities.html", "Recipe 3 Extracting Tweet Entities 3.1 Problem 3.2 Solution 3.3 Discussion 3.4 See Also", " Recipe 3 Extracting Tweet Entities 3.1 Problem You want to extract tweet entities such as @mentions, #hashtags, and short URLs from Twitter search results or other batches of tweets. 3.2 Solution Use rtweet::search_tweets() or any of the timeline functions in rtweet. 3.3 Discussion Michael has provided a very powerful search interace for Twitter data mining. rtweet::search_tweets() retrieves, parses and extracts an asounding amount of data for you to then use. Let’s search Twitter for the #rstats hashtag and see what is available: library(rtweet) library(tidyverse) (rstats &lt;- search_tweets(&quot;#rstats&quot;, n=300)) # pull 300 tweets that used the &quot;#rstats&quot; hashtag ## # A tibble: 300 x 42 ## status_id created_at user_id ## &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; ## 1 948239409790337024 2018-01-02 17:07:47 62219905 ## 2 948238781018136577 2018-01-02 17:05:17 844152803991994368 ## 3 948238133396758529 2018-01-02 17:02:43 1885980073 ## 4 948237585435983873 2018-01-02 17:00:32 2311645130 ## 5 948237466556825600 2018-01-02 17:00:04 933993004133732352 ## 6 948237231604621312 2018-01-02 16:59:08 2359597790 ## 7 948236927194542080 2018-01-02 16:57:55 944231 ## 8 948236596352114689 2018-01-02 16:56:36 734457714567438337 ## 9 948235971501481985 2018-01-02 16:54:07 847851963773460481 ## 10 948235268297052161 2018-01-02 16:51:19 1308811981 ## # ... with 290 more rows, and 39 more variables: screen_name &lt;chr&gt;, ## # text &lt;chr&gt;, source &lt;chr&gt;, reply_to_status_id &lt;chr&gt;, ## # reply_to_user_id &lt;chr&gt;, reply_to_screen_name &lt;chr&gt;, is_quote &lt;lgl&gt;, ## # is_retweet &lt;lgl&gt;, favorite_count &lt;int&gt;, retweet_count &lt;int&gt;, ## # hashtags &lt;list&gt;, symbols &lt;list&gt;, urls_url &lt;list&gt;, urls_t.co &lt;list&gt;, ## # urls_expanded_url &lt;list&gt;, media_url &lt;list&gt;, media_t.co &lt;list&gt;, ## # media_expanded_url &lt;list&gt;, media_type &lt;list&gt;, ext_media_url &lt;list&gt;, ## # ext_media_t.co &lt;list&gt;, ext_media_expanded_url &lt;list&gt;, ## # ext_media_type &lt;lgl&gt;, mentions_user_id &lt;list&gt;, ## # mentions_screen_name &lt;list&gt;, lang &lt;chr&gt;, quoted_status_id &lt;chr&gt;, ## # quoted_text &lt;chr&gt;, retweet_status_id &lt;chr&gt;, retweet_text &lt;chr&gt;, ## # place_url &lt;chr&gt;, place_name &lt;chr&gt;, place_full_name &lt;chr&gt;, ## # place_type &lt;chr&gt;, country &lt;chr&gt;, country_code &lt;chr&gt;, ## # geo_coords &lt;list&gt;, coords_coords &lt;list&gt;, bbox_coords &lt;list&gt; glimpse(rstats) ## Observations: 300 ## Variables: 42 ## $ status_id &lt;chr&gt; &quot;948239409790337024&quot;, &quot;9482387810181365... ## $ created_at &lt;dttm&gt; 2018-01-02 17:07:47, 2018-01-02 17:05:... ## $ user_id &lt;chr&gt; &quot;62219905&quot;, &quot;844152803991994368&quot;, &quot;1885... ## $ screen_name &lt;chr&gt; &quot;alisonmarigold&quot;, &quot;rweekly_live&quot;, &quot;C_Ba... ## $ text &lt;chr&gt; &quot;RT @sellorm: Introducing the Field Gui... ## $ source &lt;chr&gt; &quot;Carbon v.2&quot;, &quot;R Weekly Live&quot;, &quot;Twitter... ## $ reply_to_status_id &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;94823184946102... ## $ reply_to_user_id &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;18931434&quot;, NA,... ## $ reply_to_screen_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;RussellSPierce... ## $ is_quote &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALS... ## $ is_retweet &lt;lgl&gt; TRUE, FALSE, TRUE, FALSE, FALSE, FALSE,... ## $ favorite_count &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... ## $ retweet_count &lt;int&gt; 100, 0, 13, 0, 0, 0, 0, 1, 15, 100, 0, ... ## $ hashtags &lt;list&gt; [&quot;rstats&quot;, &lt;&quot;rstats&quot;, &quot;datascience&quot;&gt;, ... ## $ symbols &lt;list&gt; [NA, NA, NA, NA, NA, NA, NA, NA, NA, N... ## $ urls_url &lt;list&gt; [&quot;blog.sellorm.com/2018/01/01/fie\\u202... ## $ urls_t.co &lt;list&gt; [&quot;https://t.co/Hfrs1fi74u&quot;, &quot;https://t... ## $ urls_expanded_url &lt;list&gt; [&quot;http://blog.sellorm.com/2018/01/01/f... ## $ media_url &lt;list&gt; [NA, NA, &quot;http://pbs.twimg.com/media/D... ## $ media_t.co &lt;list&gt; [NA, NA, &quot;https://t.co/W7OtYESyEG&quot;, &quot;h... ## $ media_expanded_url &lt;list&gt; [NA, NA, &quot;https://twitter.com/dataandm... ## $ media_type &lt;list&gt; [NA, NA, &quot;photo&quot;, &quot;photo&quot;, &quot;photo&quot;, NA... ## $ ext_media_url &lt;list&gt; [NA, NA, &quot;http://pbs.twimg.com/media/D... ## $ ext_media_t.co &lt;list&gt; [NA, NA, &quot;https://t.co/W7OtYESyEG&quot;, &quot;h... ## $ ext_media_expanded_url &lt;list&gt; [NA, NA, &quot;https://twitter.com/dataandm... ## $ ext_media_type &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ mentions_user_id &lt;list&gt; [&quot;14351134&quot;, &quot;25213966&quot;, &quot;3230388598&quot;,... ## $ mentions_screen_name &lt;list&gt; [&quot;sellorm&quot;, &quot;MicrosoftR&quot;, &quot;dataandme&quot;,... ## $ lang &lt;chr&gt; &quot;en&quot;, &quot;en&quot;, &quot;en&quot;, &quot;en&quot;, &quot;en&quot;, &quot;en&quot;, &quot;en... ## $ quoted_status_id &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ quoted_text &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ retweet_status_id &lt;chr&gt; &quot;947909537859809281&quot;, NA, &quot;947954865464... ## $ retweet_text &lt;chr&gt; &quot;Introducing the Field Guide to the #rs... ## $ place_url &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ place_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ place_full_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ place_type &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ country &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ country_code &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ geo_coords &lt;list&gt; [&lt;NA, NA&gt;, &lt;NA, NA&gt;, &lt;NA, NA&gt;, &lt;NA, NA... ## $ coords_coords &lt;list&gt; [&lt;NA, NA&gt;, &lt;NA, NA&gt;, &lt;NA, NA&gt;, &lt;NA, NA... ## $ bbox_coords &lt;list&gt; [&lt;NA, NA, NA, NA, NA, NA, NA, NA&gt;, &lt;NA... From the output, you can see that all the URLs (short and expanded), status id’s, user id’s and other hashtags are all available and all in a tidy data frame. What are the top 10 (with ties) other hashtags used in conjunction with #rstats (for this search group)? select(rstats, hashtags) %&gt;% unnest() %&gt;% mutate(hashtags = tolower(hashtags)) %&gt;% count(hashtags, sort=TRUE) %&gt;% filter(hashtags != &quot;rstats&quot;) %&gt;% top_n(10) ## # A tibble: 11 x 2 ## hashtags n ## &lt;chr&gt; &lt;int&gt; ## 1 datascience 43 ## 2 python 19 ## 3 dataviz 12 ## 4 newyear 11 ## 5 tidyverse 8 ## 6 r 7 ## 7 ggplot2 6 ## 8 regression 6 ## 9 soccersalaries 6 ## 10 analytics 5 ## 11 bigdata 5 3.4 See Also Official Twitter search API documentation Twitter entites information The tidyverse introduction. "],
["searching-for-tweets.html", "Recipe 4 Searching for Tweets 4.1 Problem 4.2 Solution 4.3 Discussion 4.4 See Also", " Recipe 4 Searching for Tweets 4.1 Problem You want to collect a sample of tweets from the public timeline for a custom query. 4.2 Solution Use rtweet::search_tweets() and custom search operators. 4.3 Discussion The Twitter API has free and paid tiers. The free tier is what many of us use and there are a number of operators that can be added to a search query to refine the results. We saw one of those in Recipe 3 by using the #rstats hashtag in the search query. But there are far more options at our disposal. We can see all the #rstats tweets that aren’t retweets: library(rtweet) library(tidyverse) search_tweets(&quot;#rstats -filter:retweets&quot;) %&gt;% select(text) ## # A tibble: 100 x 1 ## text ## &lt;chr&gt; ## 1 &quot;New R Package for #SEO\\n\\nDiscover RsparkleR : a web crawler powered by @S ## 2 Word Embeddings with Keras https://t.co/QHzvmKNdGS #rstats https://t.co/KwD ## 3 Word Embeddings with Keras https://t.co/cbN12jx74j #rstats ## 4 You can read the first chapter of Practical Data Science with R on #liveBoo ## 5 &quot;Big | Data | Insights! https://t.co/hgMeKIOsjQ\\n#in #rstats #datascience&quot; ## 6 Do you have bad R habits? Here&#39;s how to identify and fix them. @MicrosoftR ## 7 What are the resources to learn creating #Chatbot using R? https://t.co/ip9 ## 8 I have been playing with #ggplot2 and I rather like this plot of bird densi ## 9 Linking RStudio and GitHub. Very useful blog for anyone interested in produ ## 10 &quot;@RussellSPierce @davidjayharris After using it for a long time, it&#39;s reall ## # ... with 90 more rows or, all the recent tweet-replies sent to @kearneymw: search_tweets(&quot;to:kearneymw&quot;) %&gt;% select(text) ## # A tibble: 100 x 1 ## text ## &lt;chr&gt; ## 1 @kearneymw @BreitbartNews First thing&#39;s first: Breitbart is NOT &#39;news&#39;. Na ## 2 @kearneymw @Twitter interesting. This is what I see https://t.co/PVcuXp8u0r ## 3 &quot;@kearneymw @Grantimus9 Dang, he was right again.\\n\\nDebate world is small! ## 4 @kearneymw Amazing. Indeed. @Grantimus9 ## 5 &quot;@kearneymw Really good. Thanks. \\n\\n...Eating more fish&quot; ## 6 @kearneymw Examples sing things well (cohort analysis!) always welcome ## 7 @josephofiowa I&#39;m using it as an example of (a) difficulties in making caus ## 8 @kearneymw https://t.co/JI5AZvdoTq ## 9 &quot;@kearneymw Ohhh, this is neat.\\n\\nLol @ the projections that simply omit t ## 10 @kearneymw I just Lacan&#39;t? ## # ... with 90 more rows and, even all the #rstats tweets that have GitHub links in them (but no #python hashtags): search_tweets(&quot;#rstats url:github -#python&quot;) %&gt;% select(text) ## # A tibble: 100 x 1 ## text ## &lt;chr&gt; ## 1 &quot;RT @dataandme: \\U0001f60e\\U0001f4e6 for changing up your R-\\U0001f4ca text ## 2 Linking RStudio and GitHub. Very useful blog for anyone interested in produ ## 3 &quot;New year, new blog! Find out how to build this animation that brings a tSN ## 4 &quot;Open-source #rstats package mclust provides results that are identical to ## 5 &quot;RT @dataandme: \\U0001f60e\\U0001f4e6 for changing up your R-\\U0001f4ca text ## 6 &quot;RT @dataandme: \\U0001f60e\\U0001f4e6 for changing up your R-\\U0001f4ca text ## 7 &quot;RT @ellessenne: That feeling \\U0001f525 thanks #devtools! Interested in co ## 8 &quot;RT @ma_salmon: @zevross I&#39;ll stop the link dumping I promise, but @astroer ## 9 @zevross I&#39;ll stop the link dumping I promise, but @astroeringrand&#39;s recent ## 10 RT @keyboardpipette: @RyanEs Done: https://t.co/kuLOh3liMj My first #rstats ## # ... with 90 more rows 4.4 See Also Twitter standard search operators "],
["extracting-a-retweets-origins.html", "Recipe 5 Extracting a Retweet’s Origins 5.1 Problem 5.2 Solution 5.3 Discussion 5.4 See Also", " Recipe 5 Extracting a Retweet’s Origins 5.1 Problem You want to extract the originating source from a retweet. 5.2 Solution If the tweet’s retweet_count field is greater than 0, extract name out of the tweet’s user field; also parse the text of the tweet with a regular expression. 5.3 Discussion Twitter is pretty darn good about weaponizingutilizing the data on its platform. There aren’t many cases nowadays when you need to parse RT or via in hand-crafted retweets, but it’s good to have the tools in your aresenal when needed. We can pick out all the retweets from #rstats (warning: it’s a retweet-heavy hashtag) and who they refer to using the retweet_count but also looking for a special regular expression (regex) and extracting data that way. First, the modern, API-centric way: library(rtweet) library(tidyverse) rstats &lt;- search_tweets(&quot;#rstats&quot;, n=500) glimpse(rstats) ## Observations: 500 ## Variables: 42 ## $ status_id &lt;chr&gt; &quot;948262548570177536&quot;, &quot;9482624955583692... ## $ created_at &lt;dttm&gt; 2018-01-02 18:39:44, 2018-01-02 18:39:... ## $ user_id &lt;chr&gt; &quot;22462234&quot;, &quot;22462234&quot;, &quot;3367336625&quot;, &quot;... ## $ screen_name &lt;chr&gt; &quot;bffo&quot;, &quot;bffo&quot;, &quot;HeathrTurnr&quot;, &quot;_RCharl... ## $ text &lt;chr&gt; &quot;RT @sellorm: Introducing the Field Gui... ## $ source &lt;chr&gt; &quot;Twitter for iPhone&quot;, &quot;Twitter for iPho... ## $ reply_to_status_id &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ reply_to_user_id &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ reply_to_screen_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ is_quote &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALS... ## $ is_retweet &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TR... ## $ favorite_count &lt;int&gt; 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, ... ## $ retweet_count &lt;int&gt; 112, 25, 2, 1, 1, 25, 8, 0, 112, 4, 112... ## $ hashtags &lt;list&gt; [&quot;rstats&quot;, &quot;rstats&quot;, &quot;rstats&quot;, &lt;&quot;rstat... ## $ symbols &lt;list&gt; [NA, NA, NA, NA, NA, NA, NA, NA, NA, N... ## $ urls_url &lt;list&gt; [&quot;blog.sellorm.com/2018/01/01/fie\\u202... ## $ urls_t.co &lt;list&gt; [&quot;https://t.co/Hfrs1fi74u&quot;, NA, NA, NA... ## $ urls_expanded_url &lt;list&gt; [&quot;http://blog.sellorm.com/2018/01/01/f... ## $ media_url &lt;list&gt; [NA, NA, NA, NA, &quot;http://pbs.twimg.com... ## $ media_t.co &lt;list&gt; [NA, NA, NA, NA, &quot;https://t.co/HpqJm7L... ## $ media_expanded_url &lt;list&gt; [NA, NA, NA, NA, &quot;https://twitter.com/... ## $ media_type &lt;list&gt; [NA, NA, NA, NA, &quot;photo&quot;, NA, NA, &quot;pho... ## $ ext_media_url &lt;list&gt; [NA, NA, NA, NA, &quot;http://pbs.twimg.com... ## $ ext_media_t.co &lt;list&gt; [NA, NA, NA, NA, &quot;https://t.co/HpqJm7L... ## $ ext_media_expanded_url &lt;list&gt; [NA, NA, NA, NA, &quot;https://twitter.com/... ## $ ext_media_type &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ mentions_user_id &lt;list&gt; [&quot;14351134&quot;, &quot;2167059661&quot;, &lt;&quot;567537377... ## $ mentions_screen_name &lt;list&gt; [&quot;sellorm&quot;, &quot;JennyBryan&quot;, &lt;&quot;KKulma&quot;, &quot;... ## $ lang &lt;chr&gt; &quot;en&quot;, &quot;en&quot;, &quot;en&quot;, &quot;en&quot;, &quot;en&quot;, &quot;en&quot;, &quot;en... ## $ quoted_status_id &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ quoted_text &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ retweet_status_id &lt;chr&gt; &quot;947909537859809281&quot;, &quot;9479691476058849... ## $ retweet_text &lt;chr&gt; &quot;Introducing the Field Guide to the #rs... ## $ place_url &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ place_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ place_full_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ place_type &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ country &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ country_code &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ geo_coords &lt;list&gt; [&lt;NA, NA&gt;, &lt;NA, NA&gt;, &lt;NA, NA&gt;, &lt;NA, NA... ## $ coords_coords &lt;list&gt; [&lt;NA, NA&gt;, &lt;NA, NA&gt;, &lt;NA, NA&gt;, &lt;NA, NA... ## $ bbox_coords &lt;list&gt; [&lt;NA, NA, NA, NA, NA, NA, NA, NA&gt;, &lt;NA... filter(rstats, retweet_count &gt; 0) %&gt;% select(text, mentions_screen_name, retweet_count) %&gt;% mutate(text = substr(text, 1, 30)) %&gt;% unnest() ## # A tibble: 505 x 3 ## text retweet_count mentions_screen_name ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 RT @sellorm: Introducing the F 112 sellorm ## 2 RT @JennyBryan: This overview 25 JennyBryan ## 3 RT @KKulma: Help us improve an 2 KKulma ## 4 RT @KKulma: Help us improve an 2 RLadiesLondon ## 5 RT @ma_salmon: My first #rstat 1 ma_salmon ## 6 RT @ma_salmon: My first #rstat 1 Thoughtfulnz ## 7 My first #rstats post of the y 1 Thoughtfulnz ## 8 RT @edzerpebesma: New #rspatia 25 edzerpebesma ## 9 RT @dataandme: R-code to find 8 dataandme ## 10 RT @dataandme: R-code to find 8 LVaudor ## # ... with 495 more rows The text column was pared down for display brevity. If you run that code snippet you can examine it to see that it identifies the retweets and the first screen name is usually the main reference, but you get all of the screen names from the original tweet for free. Here’s the brute-force way. A regular expression is used that matches the vast majority of retweet formats. The patten looks for them then extracts the first found screen name: # regex mod from https://stackoverflow.com/questions/655903/python-regular-expression-for-retweets filter(rstats, str_detect(text, &quot;(RT|via)((?:[[:blank:]:]\\\\W*@\\\\w+)+)&quot;)) %&gt;% select(text, mentions_screen_name, retweet_count) %&gt;% mutate(extracted = str_match(text, &quot;(RT|via)((?:[[:blank:]:]\\\\W*@\\\\w+)+)&quot;)[,3]) %&gt;% mutate(text = substr(text, 1, 30)) %&gt;% unnest() ## # A tibble: 445 x 4 ## text retweet_count ## &lt;chr&gt; &lt;int&gt; ## 1 RT @rweekly_org: https://t.co/ 23 ## 2 RT @AnalyticsVidhya: What are 4 ## 3 &quot;RT @dataandme: Still a fave \\U0001f4fd &quot; 1 ## 4 RT @sellorm: Introducing the F 108 ## 5 RT @rstudio: Word Embeddings w 4 ## 6 RT @sellorm: Introducing the F 108 ## 7 RT @AnalyticsVidhya: What are 4 ## 8 RT @StatGarrett: Thank you @co 3 ## 9 RT @StatGarrett: Thank you @co 3 ## 10 RT @StatGarrett: Thank you @co 3 ## # ... with 435 more rows, and 2 more variables: extracted &lt;chr&gt;, ## # mentions_screen_name &lt;chr&gt; You should try the above snippets for other tags as there will be cases when the regex will pick up retweets Twitter has failed to capture. 5.4 See Also Twiter official documentation on what happens to retweets when origin tweets are deleted "],
["creating-a-graph-of-retweet-relationships.html", "Recipe 6 Creating a Graph of Retweet Relationships 6.1 Problem 6.2 Solution 6.3 Discussion 6.4 See Also", " Recipe 6 Creating a Graph of Retweet Relationships 6.1 Problem You want to construct and analyze a graph data structure of retweet relationships for a set of query results. 6.2 Solution Query for the topic, extract the retweet origins, and then use igraph to construct a graph to analyze. 6.3 Discussion Recipes 4 and 5 introduced and expanded on searching Twitter plus looking for retweets. The igraph package can be used to capture and analyze details of relationships across retweets. We’ll focus on just examining the Twitter user pair relationships. Let’s get a larger sample this time — 1,500 tweets in #rstats. We can use the technique from the previous recips and: find the retweets (using the API-provided data) expand out all the mentioned screen names create an igraph graph object look at some summary statistics for the graph library(rtweet) library(igraph) library(hrbrthemes) library(tidyverse) rstats &lt;- search_tweets(&quot;#rstats&quot;, n=1500) filter(rstats, retweet_count &gt; 0) %&gt;% select(screen_name, mentions_screen_name) %&gt;% unnest(mentions_screen_name) %&gt;% filter(!is.na(mentions_screen_name)) %&gt;% graph_from_data_frame() -&gt; rt_g You can reference the igraph print() and summary() functions for more information on the output of summary() but output from the following line shows that the graph is Directed with Named vertices and it has 890 vertices and 1,487 edges. summary(rt_g) ## IGRAPH 11823c1 DN-- 890 1487 -- ## + attr: name (v/c) We’ll produce more visualizations in the next recipe, but the degree of graph vertices is one of the most fundamental properties of a graph and it’s much nicer to see the degree distribution than stare at a wall of numbers: ggplot(data_frame(y=degree_distribution(rt_g), x=1:length(y))) + geom_segment(aes(x, y, xend=x, yend=0), color=&quot;slateblue&quot;) + scale_y_continuous(expand=c(0,0), trans=&quot;sqrt&quot;) + labs(x=&quot;Degree&quot;, y=&quot;Density (sqrt scale)&quot;, title=&quot;#rstats Retweet Degree Distribution&quot;) + theme_ipsum_rc(grid=&quot;Y&quot;, axis=&quot;x&quot;) 6.4 See Also igraph "],
["visualizing-a-graph-of-retweet-relationships.html", "Recipe 7 Visualizing a Graph of Retweet Relationships 7.1 Problem 7.2 Solution 7.3 See Also", " Recipe 7 Visualizing a Graph of Retweet Relationships 7.1 Problem You want to visualize a graph of retweets. 7.2 Solution There are a plethora of ways to visualize graph structures in R. One recent and popular one is ggraph. Given the cookbook-nature of this book, we’ll cover one more visualization about retweet relationships. Let’s explore the entire retweet network and label the screen names with the most retweets over a given search term (and use #rstats again, but gather more tweets this time to truly make a spaghetti chart): library(rtweet) library(igraph) library(hrbrthemes) library(ggraph) library(tidyverse) rstats &lt;- search_tweets(&quot;#rstats&quot;, n=1500) # same as previous recipe filter(rstats, retweet_count &gt; 0) %&gt;% select(screen_name, mentions_screen_name) %&gt;% unnest(mentions_screen_name) %&gt;% filter(!is.na(mentions_screen_name)) %&gt;% graph_from_data_frame() -&gt; rt_g To help de-clutter the vertex labels, we’ll only add labels for nodes that have a degree of 20 or more (rough guess — you should look at the degree distribution for more formal work). We’ll also include the degree for those nodes so we can size them properly: V(rt_g)$node_label &lt;- unname(ifelse(degree(rt_g)[V(rt_g)] &gt; 20, names(V(rt_g)), &quot;&quot;)) V(rt_g)$node_size &lt;- unname(ifelse(degree(rt_g)[V(rt_g)] &gt; 20, degree(rt_g), 0)) Now, we’ll creatre the graph. Using ..index.. for the alpha channel will help show edge weight without too much extra effort. Note the heavy customization of geom_node_label(). Thomas made it way too easy to make beautiful network graphs with ggraph: ggraph(rt_g, layout = &#39;linear&#39;, circular = TRUE) + geom_edge_arc(edge_width=0.125, aes(alpha=..index..)) + geom_node_label(aes(label=node_label, size=node_size), label.size=0, fill=&quot;#ffffff66&quot;, segment.colour=&quot;springgreen&quot;, color=&quot;slateblue&quot;, repel=TRUE, family=font_rc, fontface=&quot;bold&quot;) + coord_fixed() + scale_size_area(trans=&quot;sqrt&quot;) + labs(title=&quot;Retweet Relationships&quot;, subtitle=&quot;Most retweeted screen names labeled. Darkers edges == more retweets. Node size == larger degree&quot;) + theme_graph(base_family=font_rc) + theme(legend.position=&quot;none&quot;) 7.3 See Also Enter twitter network analysis r into Google (seriously!). Lots of folks have worked in this space and blogged or wrote about their efforts. "],
["capturing-tweets-in-real-time-with-the-streaming-api.html", "Recipe 8 Capturing Tweets in Real-time with the Streaming API 8.1 Problem 8.2 Solution 8.3 Discussion 8.4 See Also", " Recipe 8 Capturing Tweets in Real-time with the Streaming API 8.1 Problem You want to capture a stream of public tweets in real-time, optionally filtering by select screen names or keywords in the text of the tweet. 8.2 Solution Use rtweet::stream_tweets(). 8.3 Discussion Michael has — once again — made it way too easy to work with Twitter’s API. The rtweet::stream_tweets() function has tons of handy options to help capture tweets in real time. The primary q parameter is very versatile and has four possible capture modes: The default, q = &quot;&quot;, returns a small random sample of all publicly available Twitter statuses. To filter by keyword, provide a comma separated character string with the desired phrase(s) and keyword(s). Track users by providing a comma separated list of user IDs or screen names. Use four latitude/longitude bounding box points to stream by geo location. This must be provided via a vector of length 4, e.g., c(-125, 26, -65, 49). Let’s capture one minute of tweets in the good ol’ U S of A (this is one of Michael’s examples from the manual page for rtweet::stream_tweets(). library(rtweet) library(tidyverse) stream_tweets( lookup_coords(&quot;usa&quot;), # handy helper function in rtweet verbose = FALSE, timeout = (60 * 1), ) -&gt; usa ## Found 500 records... Found 1000 records... Found 1323 records... Imported 1323 records. Simplifying... A 60 second stream resulted in well over 1,000 records. Where are they tweeting from? count(usa, place_full_name, sort=TRUE) ## # A tibble: 724 x 2 ## place_full_name n ## &lt;chr&gt; &lt;int&gt; ## 1 Los Angeles, CA 31 ## 2 Houston, TX 28 ## 3 Manhattan, NY 24 ## 4 Florida, USA 19 ## 5 Georgia, USA 18 ## 6 California, USA 16 ## 7 Pennsylvania, USA 16 ## 8 Texas, USA 15 ## 9 Charlotte, NC 14 ## 10 Chicago, IL 14 ## # ... with 714 more rows What are they tweeting about? unnest(usa, hashtags) %&gt;% count(hashtags, sort=TRUE) %&gt;% filter(!is.na(hashtags)) ## # A tibble: 289 x 2 ## hashtags n ## &lt;chr&gt; &lt;int&gt; ## 1 job 60 ## 2 CareerArc 46 ## 3 Hiring 43 ## 4 hiring 23 ## 5 Job 12 ## 6 Jobs 12 ## 7 Hospitality 7 ## 8 Transportation 6 ## 9 Healthcare 5 ## 10 Nursing 5 ## # ... with 279 more rows What app are they using? count(usa, source, sort=TRUE) ## # A tibble: 26 x 2 ## source n ## &lt;chr&gt; &lt;int&gt; ## 1 Twitter for iPhone 894 ## 2 Twitter for Android 190 ## 3 TweetMyJOBS 81 ## 4 Instagram 62 ## 5 Twitter Web Client 43 ## 6 &quot;Tweetbot for i\\u039fS&quot; 10 ## 7 Foursquare 6 ## 8 Twitter for iPad 6 ## 9 Cities 4 ## 10 SafeTweet by TweetMyJOBS 4 ## # ... with 16 more rows Michael covers the streaming topic in-depth in a vignette. 8.4 See Also Consuming streaming data "],
["making-robust-twitter-requests.html", "Recipe 9 Making Robust Twitter Requests 9.1 Problem 9.2 Solution 9.3 Discussion 9.4 See Also", " Recipe 9 Making Robust Twitter Requests 9.1 Problem You want to write a long-running script that harvests large amounts of data, such as the friend and follower ids for a very popular Twitterer; however, the Twitter API is inherently unreliable and imposes rate limits that require you to always expect the unexpected. 9.2 Solution Use rtweet. 9.3 Discussion No code examples and not much expository in this chaper (unlike it’s Python counterpart). Michael has taken much of the pain away by having the package abstract the rate-limit issues and API wonkiness away from your code. Having said that, you can work on making these Twitter scripts or other scripts more robust by wrapping potentially troublesome calls in purrr::safely()and testing for the result before continuing with data operations. 9.4 See Also purrr::safely() "],
["harvesting-tweets.html", "Recipe 10 Harvesting Tweets 10.1 Problem 10.2 Solution 10.3 Discussion", " Recipe 10 Harvesting Tweets 10.1 Problem You want to harvest and store tweets from a collection of id values, or harvest entire timelines of tweets. 10.2 Solution Use rtweet’s timeline and status functions. 10.3 Discussion Recipe 2 showed how to do this with SQLite. Unlike other API’s rtweet returns a tidy data frame which makes it easy to put data into such rectangular data stores. Rather than repeat the example, let’s take a quick look at all of the harvesting functions in rtweet: get_collections: Get collections by user or status id. get_favorites: Get tweets data for statuses favorited by one or more target users. get_followers: Get user IDs for accounts following target user. get_friends: Get user IDs of accounts followed by target user(s). get_mentions: Get mentions for the authenticating user. get_retweeters: Get user IDs of users who retweeted a given status. get_retweets: Get the most recent retweets of a specific Twitter status get_timeline: Get one or more user timelines (tweets posted by target user(s)). get_timelines: Get one or more user timelines (tweets posted by target user(s)). lookup_collections: Get collections by user or status id. lookup_coords: Get coordinates of specified location. lookup_friendships: Lookup friendship information between two specified users. lookup_statuses: Get tweets data for given statuses (status IDs). lookup_tweets: Get tweets data for given statuses (status IDs). lookup_users: Get Twitter users data for given users (user IDs or screen names). search_tweets: Get tweets data on statuses identified via search query. search_tweets2: Get tweets data on statuses identified via search query. search_users: Get users data on accounts identified via search query. stream_tweets: Collect a live stream of Twitter data. stream_tweets2: Collect a live stream of Twitter data. One handy method for exporting this rectangular tweet data to a file format virtually any collaborator can use is rtweet::write_as_csv() which saves a flattened CSV (no nested column data). "],
["creating-a-tag-cloud-from-tweet-entities.html", "Recipe 11 Creating a Tag Cloud from Tweet Entities 11.1 Problem 11.2 Solution 11.3 Discussion", " Recipe 11 Creating a Tag Cloud from Tweet Entities 11.1 Problem You want to make a meaningless word cloud. 11.2 Solution Use harvesting techniques shown in previous recipes and pass the cloud-destined entities to an R wordcloud package. 11.3 Discussion Word clouds are virtually devoid of meaning. Neiman Lab went to far as to call them harmful. But, this recipe is in the Python version of the book (figures, eh?) and this was desingned to be a 1:1 mapping of said book, so let’s proceed. The folowing uses some handy text taming and word cloud packages to make a collage from #NationalScienceFictionDay tweets: library(rtweet) library(tidytext) library(magick) library(kumojars) # hrbrmstr/kumojars library(kumo) # hrbrmstr/kumo library(tidyverse) scifi &lt;- search_tweets(&quot;#NationalScienceFictionDay&quot;, n=1500) data_frame(txt=str_replace_all(scifi$text, &quot;#NationalScienceFictionDay&quot;, &quot;&quot;)) %&gt;% unnest_tokens(word, txt) %&gt;% anti_join(stop_words, &quot;word&quot;) %&gt;% anti_join(rtweet::stopwordslangs, &quot;word&quot;) %&gt;% anti_join(data_frame(word=c(&quot;https&quot;, &quot;t.co&quot;)), &quot;word&quot;) %&gt;% # need to make a more technical stopwords list or clean up the text better filter(nchar(word)&gt;3) %&gt;% pull(word) %&gt;% paste0(collapse=&quot; &quot;) -&gt; txt cloud_img &lt;- word_cloud(txt, width=800, height=500, min_font_size=10, max_font_size=60, scale=&quot;log&quot;) image_write(cloud_img, &quot;data/wordcloud.png&quot;) But, seriously, don’t make word clouds except for fun. "],
["summarizing-link-targets.html", "Recipe 12 Summarizing Link Targets 12.1 Problem 12.2 Solution 12.3 Discussion 12.4 See Also", " Recipe 12 Summarizing Link Targets 12.1 Problem You want to summarize the text of a web page that’s indicated by a short URL in a tweet. 12.2 Solution Extract the text from the web page, and then use a natural language processing (NLP) toolkit to help you extract the most important sentences to create a machine-generated abstract. 12.3 Discussion R has more than a few NLP tools to work with. We’ll work with the LSAfun package for this exercise. As the acronym-laden package name implies, it uses Latent Semantic Analysis (LSA) to determine the most important bits in a set of text. We’ll use tweets by data journalist extraordinaire Matt Stiles ((???)). Matt works for the Los Angeles Times and I learn a ton from him on a daily basis. He’s on top of everything. Let’s summarise some news he shared recently from the New York Times, Reuters, Washington Post, Five Thirty-Eight and his employer. We’ll limit our exploration to the first three new links we find. library(rtweet) library(LSAfun) library(jerichojars) # hrbrmstr/jerichojars library(jericho) # hrbrmstr/jericho library(tidyverse) stiles &lt;- get_timeline(&quot;stiles&quot;) filter(stiles, str_detect(urls_expanded_url, &quot;nyti|reut|wapo|lat\\\\.ms|53ei&quot;)) %&gt;% # only get tweets with news links pull(urls_expanded_url) %&gt;% # extract the links flatten_chr() %&gt;% # mush them into a nice character vector head(3) %&gt;% # get the first 3 map_chr(~{ httr::GET(.x) %&gt;% # get the URL (I&#39;m lazily calling &quot;fair use&quot; here vs check robots.txt since I&#39;m suggesting you do this for your benefit vs profit) httr::content(as=&quot;text&quot;) %&gt;% # extract the HTML jericho::html_to_text() %&gt;% # strip away extraneous HTML tags LSAfun::genericSummary(k=3) %&gt;% # summarise! paste0(collapse=&quot;\\n\\n&quot;) # easier to see }) %&gt;% walk(cat) ## We will continue to put the fairness and accuracy of everything we publish above all else — and in the inevitable moments we fall short, we will continue to own up to our mistakes, and we’ll strive to do better ## ## We will continue to put the fairness and accuracy of everything we publish above all else — and in the inevitable moments we fall short, we will continue to own up to our mistakes, and we’ll strive to do better ## ## Our report is stronger than ever, thanks to investments in new forms of journalism like interactive graphics, podcasting and digital video and even greater spending in areas like investigative, international and beat reporting Trump is the 45th president of the United States, but he has spent much of his first year in office defying the conventions and norms established by the previous 44, and transforming the presidency in ways that were once unimaginable ## ## Trump essentially calls it fake, making no effort to pretend to be above it all, except to boast that he is stronger, richer, smarter and more successful than anyone else ## ## “The hope would be that given the American people’s reaction to the way he’s handled the presidency, the people running next time will run in the opposite direction Supreme Court CONFIRMED District courts Circuit courts PENDING Obama 1 3 9 9 11 Trump 1 12 7 6 43 White nominees as percent of total Reagan 94% Trump 91 Bush I 89 Bush II 83 Carter 79 Clinton 75 Obama 64 Male nominees Reagan 92% Carter 84 Bush I 81 Trump 81 Bush II 78 Clinton 71 Obama 58 Supreme Court Circuit courts District courts Obama 1 3 9 9 11 CONFIRMED PENDING CONFIRMED PENDING Trump 1 12 7 6 43 CONFIRMED PENDING CONFIRMED PENDING White nominees as percent of total Male nominees Reagan 94% Reagan 92% 91 Carter 84 Trump Bush I 89 Bush I 81 Bush II 83 Trump 81 Carter 79 Bush II 78 75 Clinton Clinton 71 Obama 64 Obama 58 Supreme Court Circuit courts District courts Obama 1 3 9 9 11 CONFIRMED PENDING CONFIRMED PENDING Trump 1 12 7 6 43 CONFIRMED PENDING CONFIRMED PENDING White nominees as percent of total Male nominees Reagan 94% Reagan 92% 91 Carter 84 Trump Bush I 89 Bush I 81 Bush II 83 Trump 81 Carter 79 Bush II 78 75 Clinton Clinton 71 Obama 64 Obama 58 Trump nominee demographics as of Nov ## ## The improvement in sentiment in the private sector shouldn’t be shocking: If you had been told that you would receive a huge tax cut and be freed from thousands of government regulations, wouldn’t you feel better about your business? As for consumers, their improved mood since the election represents a continuation of a trend that began in 2009 as the floodwaters of the financial crisis were starting to recede ## ## Additional uninsured after repeal: Uninsured, in millions, before repeal of mandate 4 7 12 12 12 12 13 Total uninsured, 2025: 28 30 31 31 44 million ’17 ’18 ’19 ’20 ’21 ’22 ’23 ’24 ’25 Additional uninsured after repeal: Uninsured, in millions, before repeal of mandate: Total uninsured, 2025: 4 7 12 12 12 12 13 44 28 30 31 31 million 2017 ’18 ’19 ’20 ’21 ’22 ’23 ’24 ’25 Sources: Kaiser Family Foundation; Congressional Budget Office More, and Whiter, Judges From Trump While the new administration has struggled to advance its legislative priorities, it has (unfortunately) excelled at another of its responsibilities: appointing judges 12.4 See Also As noted, there are other NLP packages. Check out the CRAN Task View on NLP for more resources. "],
["harvesting-friends-and-followers.html", "Recipe 13 Harvesting Friends and Followers 13.1 Problem 13.2 Solution 13.3 Discussion 13.4 See Also", " Recipe 13 Harvesting Friends and Followers 13.1 Problem You want to harvest all of the friends or followers for a particular user. 13.2 Solution Use rtweet::get_followers() or rtweet::get_friends(). 13.3 Discussion The aforementioned rtweet functions give us all the data we need and handle pagination and rate-limits. Let’s see who Brooke Anderson follows and who follows her. She’s an incredibly talented data scientist, weather expert and educator. We’ll pull her followers and friends and work with her data a bit more in future recipes. library(rtweet) library(tidyverse) (brooke_followers &lt;- rtweet::get_followers(&quot;gbwanderson&quot;)) ## # A tibble: 256 x 1 ## user_id ## &lt;chr&gt; ## 1 913819461727195138 ## 2 25819761 ## 3 2198622000 ## 4 59655036 ## 5 769616000593428480 ## 6 2973406683 ## 7 73603242 ## 8 2790116012 ## 9 392787202 ## 10 920639877397364736 ## # ... with 246 more rows (brooke_friends &lt;- rtweet::get_friends(&quot;gbwanderson&quot;)) ## # A tibble: 103 x 2 ## user user_id ## &lt;chr&gt; &lt;chr&gt; ## 1 gbwanderson 3230388598 ## 2 gbwanderson 776596392559177728 ## 3 gbwanderson 1715370056 ## 4 gbwanderson 131498466 ## 5 gbwanderson 97464922 ## 6 gbwanderson 363210621 ## 7 gbwanderson 17203405 ## 8 gbwanderson 910392773081104384 ## 9 gbwanderson 91333167 ## 10 gbwanderson 1568606814 ## # ... with 93 more rows 13.4 See Also Official Twitter API documentation on friends and followers. "],
["performing-setwise-operations-on-friendship-data.html", "Recipe 14 Performing Setwise Operations on Friendship Data 14.1 Problem 14.2 Solution 14.3 Discussion 14.4 See Also", " Recipe 14 Performing Setwise Operations on Friendship Data 14.1 Problem You want to operate on collections of friends and followers to answer questions such as “Who isn’t following me back?”, “Who are my mutual friends?”, and “What friends/followers do certain users have in common?”. 14.2 Solution Use R setwise operations amd rtweet::lookup_friendships(). 14.3 Discussion R has set operations and they’ll do just fine for helping us cook this recipe. If you need a refresher on set operations, check out this introductory lesson from Khan Academy. library(rtweet) library(tidyverse) brooke_followers &lt;- rtweet::get_followers(&quot;gbwanderson&quot;) brooke_friends &lt;- rtweet::get_friends(&quot;gbwanderson&quot;) Now we can see the count of mutual and disperate relationships: # common length(intersect(brooke_followers$user_id, brooke_friends$user_id)) ## [1] 50 # diff length(setdiff(brooke_followers$user_id, brooke_friends$user_id)) ## [1] 206 The Python counterpart to this cookbook suggests Redis as a “big-ish” data solution for performing set operations at-scale. R has at least 3 packages that provide direct support for Redis, so if you need to perform these operations at-scale, cache the info you retrieve from the Twitter API into Redis and then go crazy! 14.4 See Also Google (yes, seriously) redis packages r to see the impressive/diverse number of packages linking R to Redis Official Twitter API documentation on friends and followers. "]
]
